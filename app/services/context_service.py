"""
Context retrieval service with dynamic LLM-based query planning.

This service executes retrieval plans generated by the QueryAnalyzer.
The LLM decides:
- Which sources to query (sources with weight 0 are NEVER queried)
- What hard filters to apply (dates, entities, etc.)
- How to score/rank results (dynamic weights)

Memory Types Retrieved:
- Semantic Memory: Knowledge items with embeddings (emails, docs, tasks, events)
- Episodic Memory: Recent chat history and past conversations
- Working Memory: Current session context (via Redis)
"""

from datetime import datetime, timedelta, timezone
from typing import Union, Optional
from uuid import UUID

from sqlalchemy import select, and_, or_, func, text, cast, Text
from sqlalchemy.ext.asyncio import AsyncSession

from app.config import get_settings
from app.models import KnowledgeItem, Embedding, Entity, EntityMention, ChatSession, ChatMessage
from app.services.embedding_service import EmbeddingService
from app.services.query_analyzer import (
    QueryAnalyzer,
    RetrievalPlan,
    HardFilters,
    ScoringConfig,
    get_query_analyzer,
    QueryAnalysis,
)

settings = get_settings()


class ContextService:
    """
    Service for retrieving relevant context using dynamic LLM-generated plans.

    Key principle: The LLM's retrieval plan is AUTHORITATIVE.
    - If source weight = 0, that source is NEVER queried
    - Hard filters are ALWAYS applied (no fallback bypasses them)
    - Scoring uses the LLM's dynamic weights
    """

    def __init__(self, embedding_service: Optional[EmbeddingService] = None):
        self.embedding_service = embedding_service or EmbeddingService()
        self.query_analyzer = get_query_analyzer()

    async def retrieve_with_plan(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        session_id: Optional[str] = None,
        include_episodic: bool = True,
        limit: int = 10,
    ) -> dict:
        """
        Main retrieval method using dynamic LLM-generated plan.

        This is the new primary method that:
        1. Analyzes query with LLM to generate retrieval plan
        2. Executes plan by querying only relevant sources
        3. Applies hard filters strictly
        4. Scores results with dynamic weights
        """
        # Step 1: Get retrieval plan from LLM
        plan = await self.query_analyzer.analyze(query, str(user_id))

        # Step 2: Get active sources (weight > 0)
        active_sources = [source for source, weight in plan.sources.items() if weight > 0]

        if not active_sources:
            # Fallback: if LLM excluded everything, use gmail and gdrive
            active_sources = ["gmail", "gdrive"]

        # Step 3: Parse date filters
        date_from = None
        date_to = None
        if plan.filters.date_from:
            try:
                date_from = datetime.fromisoformat(plan.filters.date_from.replace("Z", "+00:00"))
                if date_from.tzinfo is None:
                    date_from = date_from.replace(tzinfo=timezone.utc)
            except:
                pass
        if plan.filters.date_to:
            try:
                date_to = datetime.fromisoformat(plan.filters.date_to.replace("Z", "+00:00"))
                if date_to.tzinfo is None:
                    date_to = date_to.replace(tzinfo=timezone.utc)
            except:
                pass

        # Handle future temporal queries
        if plan.temporal_direction == "future" and not date_from:
            date_from = datetime.now(timezone.utc)

        # Step 4: Execute retrieval based on strategy
        results = []

        if plan.strategy == "filter_first":
            # Prioritize hard filters - get items matching filters first
            results = await self._execute_filter_first(
                db, user_id, query, active_sources, plan, date_from, date_to, limit
            )
        elif plan.strategy == "recency_first":
            # Prioritize recent items
            results = await self._execute_recency_first(
                db, user_id, query, active_sources, plan, date_from, date_to, limit
            )
        elif plan.strategy == "semantic_first":
            # Prioritize semantic similarity
            results = await self._execute_semantic_first(
                db, user_id, query, active_sources, plan, date_from, date_to, limit
            )
        else:
            # Balanced approach - combine multiple strategies
            results = await self._execute_balanced(
                db, user_id, query, active_sources, plan, date_from, date_to, limit
            )

        # Step 5: Apply dynamic scoring
        scored_results = self._apply_dynamic_scoring(results, query, plan)

        # Step 6: Sort by score
        scored_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)

        # Step 7: Add episodic memory if requested
        if include_episodic:
            episodic = await self.get_episodic_memory(
                db, user_id, query, session_id, limit=3
            )
            scored_results.extend(episodic)

        # Step 8: Final sort and limit
        scored_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        final_results = scored_results[:limit]

        # Extract entities from results
        entities = self._extract_entities(final_results)

        return {
            "items": final_results,
            "entities": entities,
            "total": len(final_results),
            "query_analysis": {
                "sources": active_sources,
                "source_weights": plan.sources,
                "filters": {
                    "date_from": date_from.isoformat() if date_from else None,
                    "date_to": date_to.isoformat() if date_to else None,
                    "entities": plan.filters.entities,
                    "projects": plan.filters.projects,
                },
                "scoring": plan.scoring.model_dump(),
                "strategy": plan.strategy,
                "is_temporal": plan.is_temporal,
                "temporal_direction": plan.temporal_direction,
                "reasoning": plan.reasoning,
            }
        }

    async def _execute_filter_first(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        sources: list[str],
        plan: RetrievalPlan,
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Execute filter-first strategy: apply hard filters strictly."""
        results = []

        # Entity filter search
        if plan.filters.entities:
            entity_results = await self._search_by_entities(
                db, user_id, plan.filters.entities, sources, date_from, date_to, limit
            )
            results.extend(entity_results)

        # If we have date filters, get items within date range
        if date_from or date_to:
            date_results = await self._search_by_date_range(
                db, user_id, sources, date_from, date_to, limit
            )
            results.extend(date_results)

        # Add semantic search within filters
        semantic_results = await self._semantic_search(
            db, user_id, query, sources, date_from, date_to, limit
        )
        results.extend(semantic_results)

        return self._deduplicate(results)

    async def _execute_recency_first(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        sources: list[str],
        plan: RetrievalPlan,
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Execute recency-first strategy: prioritize recent items."""
        results = []

        # Get most recent items from each source
        for source in sources:
            recent = await self._get_recent_from_source(
                db, user_id, source, date_from, date_to, limit // len(sources) + 1
            )
            results.extend(recent)

        # Also add entity matches if specified
        if plan.filters.entities:
            entity_results = await self._search_by_entities(
                db, user_id, plan.filters.entities, sources, date_from, date_to, limit
            )
            results.extend(entity_results)

        return self._deduplicate(results)

    async def _execute_semantic_first(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        sources: list[str],
        plan: RetrievalPlan,
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Execute semantic-first strategy: prioritize meaning match."""
        results = []

        # Semantic search is primary
        semantic_results = await self._semantic_search(
            db, user_id, query, sources, date_from, date_to, limit * 2
        )
        results.extend(semantic_results)

        # Add entity matches
        if plan.filters.entities:
            entity_results = await self._search_by_entities(
                db, user_id, plan.filters.entities, sources, date_from, date_to, limit
            )
            results.extend(entity_results)

        # Add fulltext search
        fts_results = await self._fulltext_search(
            db, user_id, query, sources, date_from, date_to, limit
        )
        results.extend(fts_results)

        return self._deduplicate(results)

    async def _execute_balanced(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        sources: list[str],
        plan: RetrievalPlan,
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Execute balanced strategy: combine all approaches."""
        results = []

        # 1. Entity search if entities specified
        if plan.filters.entities:
            entity_results = await self._search_by_entities(
                db, user_id, plan.filters.entities, sources, date_from, date_to, limit
            )
            results.extend(entity_results)

        # 2. Semantic search
        semantic_results = await self._semantic_search(
            db, user_id, query, sources, date_from, date_to, limit
        )
        results.extend(semantic_results)

        # 3. Fulltext search
        fts_results = await self._fulltext_search(
            db, user_id, query, sources, date_from, date_to, limit
        )
        results.extend(fts_results)

        # 4. Metadata search for entities
        if plan.filters.entities:
            metadata_results = await self._metadata_search(
                db, user_id, plan.filters.entities, sources, date_from, date_to, limit
            )
            results.extend(metadata_results)

        return self._deduplicate(results)

    async def _search_by_entities(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        entities: list[str],
        sources: list[str],
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Search for items by entity references."""
        results = []

        for entity in entities:
            normalized = entity.lower().strip()

            # Find matching entities in the system
            entity_stmt = select(Entity).where(
                Entity.user_id == str(user_id),
                or_(
                    Entity.normalized_name == normalized,
                    Entity.normalized_name.contains(normalized),
                    Entity.name.ilike(f"%{entity}%"),
                )
            )
            entity_result = await db.execute(entity_stmt)
            found_entities = entity_result.scalars().all()

            if not found_entities:
                continue

            entity_ids = [e.id for e in found_entities]

            # Find knowledge items mentioning these entities
            stmt = (
                select(KnowledgeItem, EntityMention.mention_context)
                .join(EntityMention, EntityMention.knowledge_item_id == KnowledgeItem.id)
                .where(
                    KnowledgeItem.user_id == str(user_id),
                    EntityMention.entity_id.in_(entity_ids),
                    KnowledgeItem.source_type.in_(sources),
                )
            )

            if date_from:
                stmt = stmt.where(KnowledgeItem.source_created_at >= date_from)
            if date_to:
                stmt = stmt.where(KnowledgeItem.source_created_at <= date_to)

            stmt = stmt.order_by(KnowledgeItem.source_created_at.desc()).limit(limit)

            result = await db.execute(stmt)
            rows = result.all()

            for row in rows:
                results.append(self._format_result(
                    row.KnowledgeItem,
                    retrieval_method="entity",
                    entity_match=entity,
                    base_score=0.8
                ))

        return results

    async def _search_by_date_range(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        sources: list[str],
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Get items within a specific date range."""
        stmt = (
            select(KnowledgeItem)
            .where(
                KnowledgeItem.user_id == str(user_id),
                KnowledgeItem.source_type.in_(sources),
            )
        )

        if date_from:
            stmt = stmt.where(KnowledgeItem.source_created_at >= date_from)
        if date_to:
            stmt = stmt.where(KnowledgeItem.source_created_at <= date_to)

        stmt = stmt.order_by(KnowledgeItem.source_created_at.desc()).limit(limit)

        result = await db.execute(stmt)
        items = result.scalars().all()

        return [
            self._format_result(item, retrieval_method="date_range", base_score=0.7)
            for item in items
        ]

    async def _get_recent_from_source(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        source: str,
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Get most recent items from a specific source."""
        stmt = (
            select(KnowledgeItem)
            .where(
                KnowledgeItem.user_id == str(user_id),
                KnowledgeItem.source_type == source,
            )
        )

        if date_from:
            stmt = stmt.where(KnowledgeItem.source_created_at >= date_from)
        if date_to:
            stmt = stmt.where(KnowledgeItem.source_created_at <= date_to)

        stmt = stmt.order_by(KnowledgeItem.source_created_at.desc()).limit(limit)

        result = await db.execute(stmt)
        items = result.scalars().all()

        return [
            self._format_result(item, retrieval_method="recency", base_score=0.85)
            for item in items
        ]

    async def _semantic_search(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        sources: list[str],
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Perform semantic search using embeddings."""
        query_embedding = await self.embedding_service.create_embedding(query)

        if query_embedding is None:
            return []

        similarity_expr = 1 - Embedding.embedding.cosine_distance(query_embedding)

        stmt = (
            select(
                KnowledgeItem,
                Embedding.chunk_text,
                Embedding.chunk_index,
                similarity_expr.label("similarity"),
            )
            .join(Embedding, Embedding.knowledge_item_id == KnowledgeItem.id)
            .where(
                KnowledgeItem.user_id == str(user_id),
                KnowledgeItem.source_type.in_(sources),
            )
        )

        if date_from:
            stmt = stmt.where(KnowledgeItem.source_created_at >= date_from)
        if date_to:
            stmt = stmt.where(KnowledgeItem.source_created_at <= date_to)

        stmt = stmt.order_by(text("similarity DESC")).limit(limit)

        result = await db.execute(stmt)
        rows = result.all()

        return [
            self._format_result(
                row.KnowledgeItem,
                retrieval_method="semantic",
                base_score=float(row.similarity),
                chunk_text=row.chunk_text,
                chunk_index=row.chunk_index,
            )
            for row in rows
        ]

    async def _fulltext_search(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        sources: list[str],
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Perform full-text search."""
        tsvector_expr = func.to_tsvector(
            "english",
            func.coalesce(KnowledgeItem.title, "") + " " + func.coalesce(KnowledgeItem.content, "")
        )
        tsquery_expr = func.plainto_tsquery("english", query)

        stmt = (
            select(
                KnowledgeItem,
                func.ts_rank(tsvector_expr, tsquery_expr).label("rank"),
            )
            .where(
                KnowledgeItem.user_id == str(user_id),
                KnowledgeItem.source_type.in_(sources),
                tsvector_expr.op("@@")(tsquery_expr),
            )
        )

        if date_from:
            stmt = stmt.where(KnowledgeItem.source_created_at >= date_from)
        if date_to:
            stmt = stmt.where(KnowledgeItem.source_created_at <= date_to)

        stmt = stmt.order_by(text("rank DESC")).limit(limit)

        try:
            result = await db.execute(stmt)
            rows = result.all()
        except Exception:
            return []

        return [
            self._format_result(
                row.KnowledgeItem,
                retrieval_method="fulltext",
                base_score=min(0.7, float(row.rank) * 2) if row.rank else 0.5,
            )
            for row in rows
        ]

    async def _metadata_search(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        entities: list[str],
        sources: list[str],
        date_from: Optional[datetime],
        date_to: Optional[datetime],
        limit: int,
    ) -> list[dict]:
        """Search in metadata fields (from, to, assignee, etc.)."""
        results = []

        for entity in entities:
            entity_lower = entity.lower()

            # Search in metadata as text
            metadata_text = func.lower(cast(KnowledgeItem.item_metadata, Text))

            stmt = (
                select(KnowledgeItem)
                .where(
                    KnowledgeItem.user_id == str(user_id),
                    KnowledgeItem.source_type.in_(sources),
                    metadata_text.contains(entity_lower),
                )
            )

            if date_from:
                stmt = stmt.where(KnowledgeItem.source_created_at >= date_from)
            if date_to:
                stmt = stmt.where(KnowledgeItem.source_created_at <= date_to)

            stmt = stmt.order_by(KnowledgeItem.source_created_at.desc()).limit(limit)

            try:
                result = await db.execute(stmt)
                items = result.scalars().all()

                for item in items:
                    results.append(self._format_result(
                        item,
                        retrieval_method="metadata",
                        entity_match=entity,
                        base_score=0.8
                    ))
            except Exception:
                continue

        return results

    def _format_result(
        self,
        item: KnowledgeItem,
        retrieval_method: str,
        base_score: float = 0.5,
        entity_match: Optional[str] = None,
        chunk_text: Optional[str] = None,
        chunk_index: Optional[int] = None,
    ) -> dict:
        """Format a KnowledgeItem into result dict."""
        return {
            "id": str(item.id),
            "source": item.source_type,
            "source_id": item.source_id,
            "content_type": item.content_type,
            "title": item.title,
            "summary": item.summary,
            "content": chunk_text or item.content,
            "metadata": item.item_metadata,
            "source_created_at": item.source_created_at.isoformat() if item.source_created_at else None,
            "retrieval_method": retrieval_method,
            "base_score": base_score,
            "entity_match": entity_match,
            "chunk_index": chunk_index,
        }

    def _deduplicate(self, results: list[dict]) -> list[dict]:
        """Remove duplicate results, keeping highest base_score."""
        seen = {}
        for item in results:
            item_id = item["id"]
            if item_id not in seen or item.get("base_score", 0) > seen[item_id].get("base_score", 0):
                seen[item_id] = item
        return list(seen.values())

    def _apply_dynamic_scoring(
        self,
        results: list[dict],
        query: str,
        plan: RetrievalPlan,
    ) -> list[dict]:
        """Apply LLM-determined scoring weights to results."""
        now = datetime.now(timezone.utc)
        query_lower = query.lower()

        for item in results:
            # Get component scores
            semantic_score = item.get("base_score", 0.5) if item.get("retrieval_method") == "semantic" else 0.3

            # Recency score (0-1, higher for more recent)
            recency_score = 0.5
            if item.get("source_created_at"):
                try:
                    created = datetime.fromisoformat(item["source_created_at"].replace("Z", "+00:00"))
                    days_old = max(0, (now - created).days)
                    recency_score = max(0, 1 - (days_old / 365))
                except:
                    pass

            # Entity match score
            entity_score = 0.8 if item.get("entity_match") else 0.0
            if plan.filters.entities:
                # Check if any entity appears in content
                content_lower = ((item.get("title") or "") + " " + (item.get("content") or "")).lower()
                for entity in plan.filters.entities:
                    if entity.lower() in content_lower:
                        entity_score = max(entity_score, 0.6)

            # Exact match score
            exact_score = 0.0
            content_lower = ((item.get("title") or "") + " " + (item.get("content") or "")).lower()
            query_words = [w for w in query_lower.split() if len(w) >= 3]
            if query_words:
                matches = sum(1 for w in query_words if w in content_lower)
                exact_score = matches / len(query_words)

            # Source authority (all sources equal for now)
            authority_score = 0.5

            # Interaction frequency (placeholder)
            interaction_score = 0.5

            # Calculate final score using LLM weights
            final_score = (
                plan.scoring.semantic_similarity * semantic_score +
                plan.scoring.recency * recency_score +
                plan.scoring.entity_match * entity_score +
                plan.scoring.exact_match * exact_score +
                plan.scoring.source_authority * authority_score +
                plan.scoring.interaction_frequency * interaction_score
            )

            # Boost based on source weight from plan
            source = item.get("source", "")
            source_weight = plan.sources.get(source, 0.5)
            final_score *= (0.5 + source_weight * 0.5)  # Scale between 0.5x and 1x

            item["relevance_score"] = round(final_score, 3)

        return results

    async def get_episodic_memory(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        current_session_id: Optional[str] = None,
        limit: int = 5,
    ) -> list[dict]:
        """Retrieve relevant episodic memory (past conversations)."""
        # Get recent sessions
        sessions_stmt = (
            select(ChatSession)
            .where(ChatSession.user_id == str(user_id))
            .order_by(ChatSession.updated_at.desc())
            .limit(10)
        )
        sessions_result = await db.execute(sessions_stmt)
        sessions = sessions_result.scalars().all()

        relevant_memories = []
        query_lower = query.lower()

        for session in sessions:
            is_current = current_session_id and str(session.id) == current_session_id
            session_weight = 1.0 if is_current else 0.5

            messages_stmt = (
                select(ChatMessage)
                .where(ChatMessage.session_id == session.id)
                .order_by(ChatMessage.created_at.desc())
                .limit(20)
            )
            messages_result = await db.execute(messages_stmt)
            messages = messages_result.scalars().all()

            for message in messages:
                if not message.content:
                    continue

                # Simple relevance check
                content_lower = message.content.lower()
                query_words = [w for w in query_lower.split() if len(w) >= 3]
                if not query_words:
                    continue

                matches = sum(1 for w in query_words if w in content_lower)
                if matches == 0:
                    continue

                relevance = min(0.4, (matches / len(query_words)) * session_weight * 0.4)

                title = session.title or f"Chat: {message.content[:30]}..."

                relevant_memories.append({
                    "id": str(message.id),
                    "source": "episodic",
                    "source_id": str(session.id),
                    "content_type": "chat_message",
                    "title": title,
                    "summary": message.content[:200],
                    "content": message.content,
                    "metadata": {"role": message.role, "is_current_session": is_current},
                    "source_created_at": message.created_at.isoformat() if message.created_at else None,
                    "relevance_score": round(relevance, 3),
                    "retrieval_method": "episodic",
                })

        relevant_memories.sort(key=lambda x: x["relevance_score"], reverse=True)
        return relevant_memories[:limit]

    def _extract_entities(self, items: list[dict]) -> list[dict]:
        """Extract unique entities from results."""
        entities = {}

        for item in items:
            metadata = item.get("metadata", {})

            if item.get("source") in ("gmail", "outlook"):
                if "from" in metadata:
                    email = metadata["from"]
                    if email not in entities:
                        entities[email] = {
                            "type": "person",
                            "name": email.split("@")[0].title(),
                            "email": email,
                        }

            if item.get("entity_match"):
                name = item["entity_match"]
                if name not in entities:
                    entities[name] = {"type": "person", "name": name}

        return list(entities.values())

    # =========================================================================
    # BACKWARD COMPATIBILITY - Keep old methods working
    # =========================================================================

    async def retrieve_with_memory(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        session_id: Optional[str] = None,
        sources: Optional[list[str]] = None,
        time_filter: Optional[str] = None,
        entity_filter: Optional[str] = None,
        include_episodic: bool = True,
        limit: int = 10,
    ) -> dict:
        """
        Backward-compatible method that now uses the dynamic plan-based approach.
        """
        # Use the new plan-based retrieval
        return await self.retrieve_with_plan(
            db=db,
            user_id=user_id,
            query=query,
            session_id=session_id,
            include_episodic=include_episodic,
            limit=limit,
        )

    async def retrieve(
        self,
        db: AsyncSession,
        user_id: Union[str, UUID],
        query: str,
        sources: Optional[list[str]] = None,
        time_filter: Optional[str] = None,
        entity_filter: Optional[str] = None,
        date_from: Optional[datetime] = None,
        date_to: Optional[datetime] = None,
        limit: int = 10,
        explicit_sources: Optional[list[str]] = None,
    ) -> dict:
        """Backward-compatible retrieve method."""
        result = await self.retrieve_with_plan(
            db=db,
            user_id=user_id,
            query=query,
            include_episodic=False,
            limit=limit,
        )
        return result
