"""
Dynamic Query Analyzer - Fully LLM-based query understanding.

This analyzer uses an LLM to:
1. Determine which sources to query (and which to EXCLUDE entirely)
2. Extract hard filters (dates, entities, source types)
3. Generate dynamic scoring weights based on query intent
4. Decide retrieval strategy

The LLM has FULL CONTROL over the retrieval plan.
No hardcoded rules - the LLM decides everything dynamically.
"""

import json
from datetime import datetime, timezone
from typing import Optional, Any
from pydantic import BaseModel, Field

from openai import AsyncOpenAI

from app.config import get_settings

settings = get_settings()


class SourceConfig(BaseModel):
    """Configuration for a single source."""
    weight: float = Field(ge=0.0, le=1.0, description="0 = exclude entirely, >0 = include with this weight")


class HardFilters(BaseModel):
    """
    Hard filters are absolute constraints.
    Results MUST match these - they're WHERE clauses, not ranking signals.
    """
    # Date constraints
    date_from: Optional[str] = Field(None, description="ISO date string - results must be >= this date")
    date_to: Optional[str] = Field(None, description="ISO date string - results must be <= this date")

    # Entity constraints
    entities: Optional[list[str]] = Field(None, description="Person names, emails, or identifiers to filter by")

    # Project/topic constraints
    projects: Optional[list[str]] = Field(None, description="Project names or identifiers")

    # Status constraints (for tasks)
    status: Optional[list[str]] = Field(None, description="Task statuses: pending, in_progress, done, etc.")

    # Priority constraints
    priority: Optional[list[str]] = Field(None, description="Priority levels: high, medium, low, urgent, etc.")

    # Any other dynamic filters the LLM decides are needed
    custom: Optional[dict[str, Any]] = Field(None, description="Any other filters specific to this query")


class ScoringConfig(BaseModel):
    """
    Dynamic scoring weights.
    These determine how results are RANKED after filtering.
    Weights should sum to 1.0.
    """
    semantic_similarity: float = Field(0.0, ge=0.0, le=1.0, description="Weight for embedding similarity")
    recency: float = Field(0.0, ge=0.0, le=1.0, description="Weight for how recent the item is")
    entity_match: float = Field(0.0, ge=0.0, le=1.0, description="Weight for entity/person matches")
    exact_match: float = Field(0.0, ge=0.0, le=1.0, description="Weight for exact keyword matches")
    source_authority: float = Field(0.0, ge=0.0, le=1.0, description="Weight for source trustworthiness")
    interaction_frequency: float = Field(0.0, ge=0.0, le=1.0, description="Weight for user interaction history")


class RetrievalPlan(BaseModel):
    """
    Complete retrieval plan generated by LLM.
    This tells the system exactly how to fetch and rank results.
    """
    # Which sources to query and their weights (0 = skip entirely)
    sources: dict[str, float] = Field(
        description="Source weights. 0 means DON'T QUERY this source at all."
    )

    # Hard filters - results MUST match these
    filters: HardFilters = Field(default_factory=HardFilters)

    # Scoring weights for ranking
    scoring: ScoringConfig = Field(default_factory=ScoringConfig)

    # Retrieval strategy hints
    strategy: str = Field(
        "balanced",
        description="Strategy: 'filter_first' (apply filters strictly), 'semantic_first' (prioritize meaning), 'recency_first' (prioritize time), 'balanced' (mix)"
    )

    # Whether this is a temporal query (asking for recent/upcoming items)
    is_temporal: bool = Field(False, description="True if query asks for recent/latest/upcoming items")

    # Temporal direction
    temporal_direction: Optional[str] = Field(None, description="'past' for recent/last, 'future' for upcoming/next")

    # Explanation for debugging
    reasoning: str = Field("", description="Brief explanation of choices")


SYSTEM_PROMPT = """You are a query analyzer for an AI assistant. Your job is to analyze user queries and generate a retrieval plan.

## Available Sources
- gmail: Emails (fields: from, to, subject, body, date, attachments, cc, bcc)
- gdrive: Google Drive documents (fields: title, content, type, created_date, modified_date, owner)
- calendar: Calendar events (fields: title, description, start_time, end_time, attendees, location, organizer)
- jira: Tasks/tickets (fields: title, description, status, assignee, reporter, priority, project, due_date, labels)
- slack: Slack messages (fields: channel, sender, content, timestamp, thread_id)
- notion: Notion pages (fields: title, content, type, created_date, tags, parent)

## Scoring Components
- semantic_similarity: How semantically related content is to the query meaning
- recency: How recent/new the content is (higher for newer)
- entity_match: How well entities (people, projects) match the query
- exact_match: Exact keyword/phrase presence
- source_authority: Importance/trustworthiness of the source
- interaction_frequency: How often user interacts with this content/person

## CRITICAL RULES

### Rule 1: Source Exclusion
If a source is NOT relevant to the query, set its weight to 0. It will NOT be queried at all.
- "show me emails" → gmail: 1.0, all others: 0.0
- "tasks assigned to Mike" → jira: 1.0, all others: 0.0
- "what's in the architecture doc" → gdrive: 1.0, all others: 0.0
- "upcoming meetings" → calendar: 1.0, all others: 0.0

### Rule 2: Hard Filters vs Ranking
- Hard filters are ABSOLUTE. Results MUST match them. Use for:
  - Specific date ranges ("November 2025", "last week")
  - Specific people ("from Sarah", "assigned to Mike")
  - Specific sources when explicitly mentioned

- Scoring weights affect RANKING only. Results are scored after filtering.

### Rule 3: Date Handling
- When user mentions a date/time range, it's a HARD FILTER, not a ranking signal
- "November emails" → filter: date_from: "2025-11-01", date_to: "2025-11-30"
- "last week" → filter: date_from: [7 days ago], date_to: [now]
- "coming up" / "upcoming" → filter: date_from: [now], temporal_direction: "future"

### Rule 4: Entity Handling
- Extract person names and resolve them consistently
- "from John" → entities: ["john", "john smith"] (include variations)
- Email addresses should be included: "sarah@client.com" → entities: ["sarah@client.com", "sarah"]

### Rule 5: Scoring Must Sum to ~1.0
The scoring weights should approximately sum to 1.0 for proper ranking.

### Rule 6: Strategy Selection
- filter_first: When user has explicit constraints (dates, specific sources, specific people)
- semantic_first: When user asks about meaning/content ("what did X say about Y")
- recency_first: When user asks for latest/recent/newest
- balanced: General queries without strong constraints

## Output Format
Return a JSON object with this exact structure:
{
    "sources": {
        "gmail": <0.0-1.0>,
        "gdrive": <0.0-1.0>,
        "calendar": <0.0-1.0>,
        "jira": <0.0-1.0>,
        "slack": <0.0-1.0>,
        "notion": <0.0-1.0>
    },
    "filters": {
        "date_from": "<ISO date or null>",
        "date_to": "<ISO date or null>",
        "entities": ["<list of names/emails>"] or null,
        "projects": ["<list of project names>"] or null,
        "status": ["<list of statuses>"] or null,
        "priority": ["<list of priorities>"] or null,
        "custom": {} or null
    },
    "scoring": {
        "semantic_similarity": <0.0-1.0>,
        "recency": <0.0-1.0>,
        "entity_match": <0.0-1.0>,
        "exact_match": <0.0-1.0>,
        "source_authority": <0.0-1.0>,
        "interaction_frequency": <0.0-1.0>
    },
    "strategy": "filter_first|semantic_first|recency_first|balanced",
    "is_temporal": true|false,
    "temporal_direction": "past|future|null",
    "reasoning": "<brief explanation>"
}

Only output valid JSON. No markdown, no explanation outside the JSON."""


USER_PROMPT_TEMPLATE = """Analyze this query and generate a retrieval plan.

Query: "{query}"

Current date/time: {current_time}

Additional context:
- User ID: {user_id}
- This user has data in: gmail, gdrive, calendar, jira

Generate the retrieval plan JSON:"""


class QueryAnalyzer:
    """
    Fully LLM-based query analyzer.

    The LLM has complete control over:
    - Which sources to query
    - What filters to apply
    - How to score/rank results
    """

    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.openai_api_key)
        self.model = "gpt-4o-mini"  # Fast and cost-effective

    async def analyze(self, query: str, user_id: str = "default") -> RetrievalPlan:
        """
        Analyze a query and generate a complete retrieval plan.

        Args:
            query: The user's natural language query
            user_id: User identifier for context

        Returns:
            RetrievalPlan with sources, filters, scoring, and strategy
        """
        current_time = datetime.now(timezone.utc).isoformat()

        user_prompt = USER_PROMPT_TEMPLATE.format(
            query=query,
            user_id=user_id,
            current_time=current_time
        )

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,  # Low temperature for consistency
                max_tokens=1000
            )

            content = response.choices[0].message.content
            plan_data = json.loads(content)

            # Parse into structured objects
            filters = HardFilters(
                date_from=plan_data.get("filters", {}).get("date_from"),
                date_to=plan_data.get("filters", {}).get("date_to"),
                entities=plan_data.get("filters", {}).get("entities"),
                projects=plan_data.get("filters", {}).get("projects"),
                status=plan_data.get("filters", {}).get("status"),
                priority=plan_data.get("filters", {}).get("priority"),
                custom=plan_data.get("filters", {}).get("custom"),
            )

            scoring_data = plan_data.get("scoring", {})
            scoring = ScoringConfig(
                semantic_similarity=scoring_data.get("semantic_similarity", 0.4),
                recency=scoring_data.get("recency", 0.2),
                entity_match=scoring_data.get("entity_match", 0.2),
                exact_match=scoring_data.get("exact_match", 0.1),
                source_authority=scoring_data.get("source_authority", 0.05),
                interaction_frequency=scoring_data.get("interaction_frequency", 0.05),
            )

            return RetrievalPlan(
                sources=plan_data.get("sources", self._default_sources()),
                filters=filters,
                scoring=scoring,
                strategy=plan_data.get("strategy", "balanced"),
                is_temporal=plan_data.get("is_temporal", False),
                temporal_direction=plan_data.get("temporal_direction"),
                reasoning=plan_data.get("reasoning", "")
            )

        except Exception as e:
            # Fallback to default plan if LLM fails
            print(f"Query analysis failed: {e}")
            return self._default_plan(query)

    def _default_sources(self) -> dict[str, float]:
        """Default source weights when LLM fails."""
        return {
            "gmail": 0.3,
            "gdrive": 0.3,
            "calendar": 0.2,
            "jira": 0.2,
            "slack": 0.0,
            "notion": 0.0,
        }

    def _default_plan(self, query: str) -> RetrievalPlan:
        """Fallback plan when LLM analysis fails."""
        return RetrievalPlan(
            sources=self._default_sources(),
            filters=HardFilters(),
            scoring=ScoringConfig(
                semantic_similarity=0.4,
                recency=0.2,
                entity_match=0.2,
                exact_match=0.1,
                source_authority=0.05,
                interaction_frequency=0.05,
            ),
            strategy="balanced",
            is_temporal=False,
            temporal_direction=None,
            reasoning="Fallback plan due to analysis failure"
        )


# Singleton instance
_analyzer: Optional[QueryAnalyzer] = None


def get_query_analyzer() -> QueryAnalyzer:
    """Get singleton query analyzer instance."""
    global _analyzer
    if _analyzer is None:
        _analyzer = QueryAnalyzer()
    return _analyzer


# Keep backward compatibility with old QueryAnalysis dataclass
from dataclasses import dataclass, field

@dataclass
class QueryAnalysis:
    """Legacy dataclass for backward compatibility."""
    query: str
    entities: list[str] = field(default_factory=list)
    sources: list[str] = field(default_factory=list)
    date_from: Optional[datetime] = None
    date_to: Optional[datetime] = None
    time_type: Optional[str] = None
    intent: str = "search"
    is_temporal: bool = False
    confidence: float = 0.0

    @classmethod
    def from_retrieval_plan(cls, query: str, plan: RetrievalPlan) -> "QueryAnalysis":
        """Convert RetrievalPlan to legacy QueryAnalysis format."""
        # Get active sources (weight > 0)
        active_sources = [s for s, w in plan.sources.items() if w > 0]

        # Parse dates
        date_from = None
        date_to = None
        if plan.filters.date_from:
            try:
                date_from = datetime.fromisoformat(plan.filters.date_from.replace("Z", "+00:00"))
            except:
                pass
        if plan.filters.date_to:
            try:
                date_to = datetime.fromisoformat(plan.filters.date_to.replace("Z", "+00:00"))
            except:
                pass

        return cls(
            query=query,
            entities=plan.filters.entities or [],
            sources=active_sources,
            date_from=date_from,
            date_to=date_to,
            time_type=plan.temporal_direction,
            intent="search",
            is_temporal=plan.is_temporal,
            confidence=0.9  # High confidence since LLM analyzed
        )
